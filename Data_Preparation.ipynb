{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szypveleV-5C"
   },
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rw1H2ndY5QfJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOn1Ce5nV-5G"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrV9I7q136Wk"
   },
   "outputs": [],
   "source": [
    "# path for speech emotion data\n",
    "\n",
    "Ravdess = \"/content/drive/MyDrive/SER/Ravdess/audio_speech_actors_01-24/\"\n",
    "# Crema = \"/content/drive/MyDrive/SER/Crema/\"\n",
    "Tess = \"/content/drive/MyDrive/SER/Tess/\"\n",
    "Savee = \"/content/drive/MyDrive/SER/Savee/\"\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in ravdess_directory_list:\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0] # e.g., 03-01-06-01-01-01-01\n",
    "        part = part.split('-')\n",
    "        # extract the third section as emotion\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', \\\n",
    "                             7:'disgust', 8:'surprise'}, inplace=True)\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "# crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "# file_emotion = []\n",
    "# file_path = []\n",
    "\n",
    "# for file in crema_directory_list:\n",
    "#     file_path.append(Crema + file)\n",
    "\n",
    "#     part = file.split('_')[2]\n",
    "#     if part == 'SAD':\n",
    "#         file_emotion.append('sad')\n",
    "#     elif part == 'ANG':\n",
    "#         file_emotion.append('angry')\n",
    "#     elif part == 'DIS':\n",
    "#         file_emotion.append('disgust')\n",
    "#     elif part == 'FEA':\n",
    "#         file_emotion.append('fear')\n",
    "#     elif part == 'HAP':\n",
    "#         file_emotion.append('happy')\n",
    "#     elif part == 'NEU':\n",
    "#         file_emotion.append('neutral')\n",
    "#     else:\n",
    "#         file_emotion.append('Unknown')\n",
    "\n",
    "# emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "# path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "# Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "tess_directory_list = os.listdir(Tess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part == 'ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)\n",
    "\n",
    "    part = file.split('_')[1][:2]\n",
    "    if part.__contains__('su'):\n",
    "        file_emotion.append('surprise')\n",
    "    elif part[0] == 'a':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[0] == 'h':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[0] == 's':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[0] == 'n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif part[0] == 'f':\n",
    "        file_emotion.append('fear')\n",
    "    else:\n",
    "        file_emotion.append('disgust')\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "data_path = pd.concat([Ravdess_df, Tess_df, Savee_df], axis=0, ignore_index=True)\n",
    "data_path.Emotions.replace({'neutral':1, 'calm':2, 'happy':3, 'sad':4, 'angry':5, 'fear':6, \\\n",
    "                            'disgust':7, 'surprise':0}, inplace=True)\n",
    "# data_path.Emotions.replace({'neutral':1, 'happy':2, 'sad':3, 'angry':4, 'fear':5, \\\n",
    "#                             'disgust':6, 'surprise':0}, inplace=True)\n",
    "\n",
    "#data_path = data_path[data_path.Emotions.isin([7, 3, 4, 6, 5, 1])]\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArhBUlaesmAe"
   },
   "outputs": [],
   "source": [
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'}\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x=range(8), height=data_path['Emotions'].value_counts())\n",
    "ax.set_xticks(ticks=range(8))\n",
    "ax.set_xticklabels([EMOTIONS[i] for i in range(8)],fontsize=10)\n",
    "ax.set_xlabel('Emotion')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgPGPSGmV-5I"
   },
   "outputs": [],
   "source": [
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(x=data_path[\"Emotions\"])\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMyKB2JOV-5N"
   },
   "source": [
    "# Load the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JekgcYaz36Wm"
   },
   "outputs": [],
   "source": [
    "mel_spectrograms = []\n",
    "\n",
    "SAMPLE_RATE = 48000\n",
    "\n",
    "savee_signals = []\n",
    "for i, file_path in enumerate(Savee_df.Path):\n",
    "    audio, sample_rate = librosa.load(file_path, duration=2, offset=0.3, sr=SAMPLE_RATE)\n",
    "    signal = np.zeros((int(SAMPLE_RATE*2,)))\n",
    "    signal[:len(audio)] = audio[:SAMPLE_RATE*2]\n",
    "    savee_signals.append(signal)\n",
    "    print(\"\\r Processed {}/{} {} files\".format(i,len(Savee_df), \"Savee\"),end='')\n",
    "savee_signals = np.stack(savee_signals,axis=0)\n",
    "\n",
    "tess_signals = []\n",
    "for i, file_path in enumerate(Tess_df.Path):\n",
    "    audio, sample_rate = librosa.load(file_path, duration=2, offset=0, sr=SAMPLE_RATE)\n",
    "    signal = np.zeros((int(SAMPLE_RATE*2,)))\n",
    "    signal[:len(audio)] = audio\n",
    "    tess_signals.append(signal)\n",
    "    print(\"\\r Processed {}/{} {} files\".format(i,len(Tess_df), \"Tess\"),end='')\n",
    "tess_signals = np.stack(tess_signals,axis=0)\n",
    "\n",
    "ravdess_signals = []\n",
    "for i, file_path in enumerate(Ravdess_df.Path):\n",
    "    audio, sample_rate = librosa.load(file_path, duration=2, offset=0.75, sr=SAMPLE_RATE)\n",
    "    signal = np.zeros((int(SAMPLE_RATE*2,)))\n",
    "    signal[:len(audio)] = audio\n",
    "    ravdess_signals.append(signal)\n",
    "    print(\"\\r Processed {}/{} {} files\".format(i,len(Ravdess_df), \"Ravdess\"),end='')\n",
    "ravdess_signals = np.stack(ravdess_signals,axis=0)\n",
    "\n",
    "total_signals = np.vstack((ravdess_signals, tess_signals, savee_signals))\n",
    "total_signals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5v34PbWV-5O"
   },
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pl5ZXsqBV-5O"
   },
   "outputs": [],
   "source": [
    "X = total_signals\n",
    "train_ind,test_ind,val_ind = [],[],[]\n",
    "X_train,X_val,X_test = [],[],[]\n",
    "Y_train,Y_val,Y_test = [],[],[]\n",
    "for emotion in range(len(EMOTIONS)):\n",
    "    emotion_ind = list(data_path.loc[data_path.Emotions==emotion,'Emotions'].index)\n",
    "    emotion_ind = np.random.permutation(emotion_ind)\n",
    "    m = len(emotion_ind)\n",
    "    ind_train = emotion_ind[:int(0.8*m)]\n",
    "    ind_val = emotion_ind[int(0.8*m):int(0.9*m)]\n",
    "    ind_test = emotion_ind[int(0.9*m):]\n",
    "\n",
    "    X_train.append(X[ind_train,:])\n",
    "    Y_train.append(np.array([emotion]*len(ind_train),dtype=np.int32))\n",
    "    X_val.append(X[ind_val,:])\n",
    "    Y_val.append(np.array([emotion]*len(ind_val),dtype=np.int32))\n",
    "    X_test.append(X[ind_test,:])\n",
    "    Y_test.append(np.array([emotion]*len(ind_test),dtype=np.int32))\n",
    "    train_ind.append(ind_train)\n",
    "    test_ind.append(ind_test)\n",
    "    val_ind.append(ind_val)\n",
    "X_train = np.concatenate(X_train,0)\n",
    "X_val = np.concatenate(X_val,0)\n",
    "X_test = np.concatenate(X_test,0)\n",
    "Y_train = np.concatenate(Y_train,0)\n",
    "Y_val = np.concatenate(Y_val,0)\n",
    "Y_test = np.concatenate(Y_test,0)\n",
    "train_ind = np.concatenate(train_ind,0)\n",
    "val_ind = np.concatenate(val_ind,0)\n",
    "test_ind = np.concatenate(test_ind,0)\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n",
    "# check if all are unique\n",
    "unique, count = np.unique(np.concatenate([train_ind,test_ind,val_ind],0), return_counts=True)\n",
    "print(\"Number of unique indexes is {}, out of {}\".format(sum(count==1), X.shape[0]))\n",
    "\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSXSjh4eV-5Q"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQqHDRhsV-5Q"
   },
   "outputs": [],
   "source": [
    "def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30):\n",
    "    signal_len = len(signal)\n",
    "    # Generate White Gaussian noise\n",
    "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
    "    # Normalize signal and noise\n",
    "    norm_constant = 2.0**(num_bits-1)\n",
    "    signal_norm = signal / norm_constant\n",
    "    noise_norm = noise / norm_constant\n",
    "    # Compute signal and noise power\n",
    "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
    "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
    "    # Random SNR: Uniform [15, 30] in dB\n",
    "    target_snr = np.random.randint(snr_low, snr_high)\n",
    "    # Compute K (covariance matrix) for each noise\n",
    "    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n",
    "    K = np.ones((signal_len, augmented_num)) * K\n",
    "    # Generate noisy signal\n",
    "    return signal + K.T * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV9T6SyYV-5R"
   },
   "outputs": [],
   "source": [
    "aug_signals = []\n",
    "aug_labels = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    signal = X_train[i,:]\n",
    "    augmented_signals = addAWGN(signal)\n",
    "    for j in range(augmented_signals.shape[0]):\n",
    "        aug_labels.append(data_path.loc[i,\"Emotions\"])\n",
    "        aug_signals.append(augmented_signals[j,:])\n",
    "        data_path = data_path.append(data_path.iloc[i], ignore_index=True)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "aug_signals = np.stack(aug_signals,axis=0)\n",
    "X_train = np.concatenate([X_train,aug_signals],axis=0)\n",
    "aug_labels = np.stack(aug_labels,axis=0)\n",
    "Y_train = np.concatenate([Y_train,aug_labels])\n",
    "print('')\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGW-G_tBV-5S"
   },
   "source": [
    "# Calculate mel spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HxpDFl5V-5S"
   },
   "outputs": [],
   "source": [
    "def getMELspectrogram(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
    "                          sr=sample_rate,\n",
    "                          n_fft=1024,\n",
    "                          win_length = 512,\n",
    "                          window='hamming',\n",
    "                          n_mels=148,\n",
    "                          fmax=sample_rate/2\n",
    "                          )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# test function\n",
    "emotion = 1\n",
    "path = np.array(data_path.Path[data_path.Emotions==emotion])[50]\n",
    "print(path)\n",
    "audio, sample_rate = librosa.load(path, duration=2, offset=0.75, sr=SAMPLE_RATE)\n",
    "signal[:len(audio)] = audio[:SAMPLE_RATE*2]\n",
    "mel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\n",
    "librosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\n",
    "print('MEL spectrogram shape: ',mel_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OC1oMxa36Wo"
   },
   "outputs": [],
   "source": [
    "mel_train = []\n",
    "print(\"Calculatin mel spectrograms for train set\")\n",
    "for i in range(X_train.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_train.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "print('')\n",
    "mel_train = np.stack(mel_train,axis=0)\n",
    "del X_train\n",
    "X_train = mel_train\n",
    "\n",
    "mel_val = []\n",
    "print(\"Calculatin mel spectrograms for val set\")\n",
    "for i in range(X_val.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_val.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_val.shape[0]),end='')\n",
    "print('')\n",
    "mel_val = np.stack(mel_val,axis=0)\n",
    "del X_val\n",
    "X_val = mel_val\n",
    "\n",
    "mel_test = []\n",
    "print(\"Calculatin mel spectrograms for test set\")\n",
    "for i in range(X_test.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_test.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\n",
    "print('')\n",
    "mel_test = np.stack(mel_test,axis=0)\n",
    "del X_test\n",
    "X_test = mel_test\n",
    "\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc3aH9A2V-5Y"
   },
   "source": [
    "stack data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKbcznwkV-5Z"
   },
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train,1)\n",
    "X_val = np.expand_dims(X_val,1)\n",
    "X_test = np.expand_dims(X_test,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "b,c,h,w = X_train.shape\n",
    "X_train = np.reshape(X_train, newshape=(b,-1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, newshape=(b,c,h,w))\n",
    "print('Shape of X_train: ',X_train.shape)\n",
    "\n",
    "b,c,h,w = X_test.shape\n",
    "X_test = np.reshape(X_test, newshape=(b,-1))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = np.reshape(X_test, newshape=(b,c,h,w))\n",
    "print('Shape of X_test: ',X_test.shape)\n",
    "\n",
    "b,c,h,w = X_val.shape\n",
    "X_val = np.reshape(X_val, newshape=(b,-1))\n",
    "X_val = scaler.transform(X_val)\n",
    "X_val = np.reshape(X_val, newshape=(b,c,h,w))\n",
    "print('Shape of X_val: ',X_val.shape)\n",
    "\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/xtrain.npy\", arr=X_train)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/xval.npy\", arr=X_val)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/xtest.npy\", arr=X_test)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/ytrain.npy\", arr=Y_train)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/yval.npy\", arr=Y_val)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/ytest.npy\", arr=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9YRPaLA36Wp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
