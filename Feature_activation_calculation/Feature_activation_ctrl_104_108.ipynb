{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fYFLFMFXSZk"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ya5oH_vGwJeW"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ibxO_Hs2fsa"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CvOXDYwXSZn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as mpl_color_map\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIX_bfpcXSZ3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.GELU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.gelu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqSv5o2qvcDw"
   },
   "outputs": [],
   "source": [
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self, block, layers, num_emotions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Resnet Block\n",
    "        self.inplanes = 64\n",
    "        self.res_conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(1, 64, kernel_size = 3, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.GELU())\n",
    "        self.res_maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.res_layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.res_layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.res_layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.res_layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.res_avgpool = nn.AvgPool2d(3, stride=1)\n",
    "#         self.res_fc = nn.Linear(512, num_emotions)\n",
    "\n",
    "        # Block 1:\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Block 2:\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,\n",
    "                               out_channels=32,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "\n",
    "        # Block 3:\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "\n",
    "        # Block 4:\n",
    "        self.conv4 = nn.Conv2d(in_channels=64,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.max_pool4 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "\n",
    "        # LSTM block\n",
    "        self.lstm_maxpool = nn.MaxPool2d(kernel_size=[2,4], stride=[2,4])\n",
    "        hidden_size = 148\n",
    "        self.lstm = nn.LSTM(input_size=74,hidden_size=hidden_size,bidirectional=True, batch_first=True)\n",
    "        self.dropout_lstm = nn.Dropout(0.1)\n",
    "        self.attention_linear = nn.Linear(2*hidden_size,1) # 2*hidden_size for the 2 outputs of bidir LSTM\n",
    "\n",
    "        # Transformer block\n",
    "        self.transf_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
    "        transf_layer = nn.TransformerEncoderLayer(d_model=148, nhead=4, dim_feedforward=512, dropout=0.4, activation='relu')\n",
    "        self.transf_encoder = nn.TransformerEncoder(transf_layer, num_layers=4)\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(2*hidden_size+212+15360,num_emotions)\n",
    "        self.dropout_linear = nn.Dropout(p=0)\n",
    "        self.out_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # resnet embedding\n",
    "        res_embedding = self.res_conv1(x)\n",
    "        res_embedding = self.res_maxpool(res_embedding)\n",
    "        res_embedding = self.res_layer0(res_embedding)\n",
    "        res_embedding = self.res_layer1(res_embedding)\n",
    "        res_embedding = self.dropout(res_embedding)\n",
    "        res_embedding = self.res_layer2(res_embedding)\n",
    "        res_embedding = self.res_layer3(res_embedding)\n",
    "        res_embedding = self.dropout(res_embedding)\n",
    "\n",
    "        # res_embedding = self.res_avgpool(res_embedding)\n",
    "        res_embedding = res_embedding.view(res_embedding.size(0), -1)\n",
    "\n",
    "        # conv embedding\n",
    "#         conv_embedding = self.conv2Dblock(x) #(b,channel,freq,time)\n",
    "        conv_embedding = self.dropout(self.max_pool1(self.relu(self.bn1(self.conv1(x)))))\n",
    "        conv_embedding = self.dropout(self.max_pool2(self.relu(self.bn2(self.conv2(conv_embedding)))))\n",
    "        conv_embedding = self.dropout(self.max_pool3(self.relu(self.bn3(self.conv3(conv_embedding)))))\n",
    "        conv_embedding = self.dropout(self.max_pool4(self.relu(self.bn4(self.conv4(conv_embedding)))))\n",
    "\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=1) # do not flatten batch dimension\n",
    "\n",
    "        # lstm embedding\n",
    "        x_reduced = self.lstm_maxpool(x)\n",
    "        x_reduced = torch.squeeze(x_reduced,1)\n",
    "        x_reduced = x_reduced.permute(0,2,1) # (batch,time,freq)\n",
    "        lstm_embedding, (h,c) = self.lstm(x_reduced) # (b, time, hidden_size*2)\n",
    "        lstm_embedding = self.dropout_lstm(lstm_embedding)\n",
    "        batch_size,T,_ = lstm_embedding.shape\n",
    "        attention_weights = [None]*T\n",
    "        for t in range(T):\n",
    "            embedding = lstm_embedding[:,t,:]\n",
    "            attention_weights[t] = self.attention_linear(embedding)\n",
    "        attention_weights_norm = nn.functional.softmax(torch.stack(attention_weights,-1),-1)\n",
    "        attention = torch.bmm(attention_weights_norm,lstm_embedding) # (Bx1xT)*(B,T,hidden_size*2)=(B,1,2*hidden_size)\n",
    "        attention = torch.squeeze(attention, 1)\n",
    "\n",
    "\n",
    "        # transformer embedding\n",
    "        x_reduced_t = self.transf_maxpool(x)\n",
    "        x_reduced_t = torch.squeeze(x_reduced_t,1)\n",
    "        x_reduced_t = x_reduced_t.permute(2,0,1) # (time,batch,embedding)\n",
    "        transf_out = self.transf_encoder(x_reduced_t)\n",
    "        transf_embedding = torch.mean(transf_out, dim=0)\n",
    "        # concatenate\n",
    "        complete_embedding = torch.cat([res_embedding, conv_embedding, attention, transf_embedding], dim=1)\n",
    "        # final Linear\n",
    "        output_logits = self.out_linear(complete_embedding)\n",
    "        output_logits = self.dropout_linear(output_logits)\n",
    "        output_softmax = self.out_softmax(output_logits)\n",
    "        return output_logits, output_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu9qyU6JvcDy"
   },
   "outputs": [],
   "source": [
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device is {}'.format(device))\n",
    "\n",
    "show_model = ParallelModel(ResidualBlock, [2,2,2,2], len(EMOTIONS))\n",
    "show_model.to(device)\n",
    "# print(show_model)\n",
    "\n",
    "\n",
    "X_train = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/xtrain.npy\")\n",
    "X_test = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/xtest.npy\")\n",
    "X_val = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/xval.npy\")\n",
    "\n",
    "Y_train = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/ytrain.npy\")\n",
    "Y_test = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/ytest.npy\")\n",
    "Y_val = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/yval.npy\")\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dLJPgBrXSZ-"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SxuUb9BXSZ-"
   },
   "outputs": [],
   "source": [
    "LOAD_PATH = os.path.join(os.getcwd(),'/content/drive/MyDrive/serdl/notebooks/new_models')\n",
    "model = ParallelModel(ResidualBlock, [2,2,2,2], len(EMOTIONS))\n",
    "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'big_ctrl_mel_model.pt')))\n",
    "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'big_ctrl_mel_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqm2lHIBvcDz"
   },
   "source": [
    "# Feature Activation Visualization (only for CNN-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NUvwoTIvcDz"
   },
   "outputs": [],
   "source": [
    "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
    "    \"\"\"\n",
    "        Apply heatmap on image\n",
    "    Args:\n",
    "        org_img (PIL img): Original image\n",
    "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
    "        colormap_name (str): Name of the colormap\n",
    "    \"\"\"\n",
    "    # Get colormap\n",
    "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
    "    no_trans_heatmap = color_map(activation)\n",
    "    # Change alpha channel in colormap to make sure original image is displayed\n",
    "    heatmap = copy.copy(no_trans_heatmap)\n",
    "    heatmap[:, :, 3] = 0.4\n",
    "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
    "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
    "\n",
    "    # Apply heatmap on iamge\n",
    "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
    "    return no_trans_heatmap, heatmap_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DawcDXWsvcDz"
   },
   "outputs": [],
   "source": [
    "gradients = []  # A gloabl variable used to save the gradient\n",
    "def generate_afm(model, input_image):\n",
    "    \n",
    "    def forward_hook(module, f_input, f_output):\n",
    "        \"\"\"\n",
    "        A function to get feature maps in forward calculation\n",
    "\n",
    "        Inputs\n",
    "        - module: non-modified system variable\n",
    "        - f_input: feature map generated by last conv layer\n",
    "        - f_output: feature map generated by this conv layer\n",
    "\n",
    "        Append feature maps generated by this conv layer to a list each time\n",
    "        \"\"\"\n",
    "        feature_map_block.append(f_output)\n",
    "\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        \"\"\"\n",
    "        A function to get gradients in back-propagation\n",
    "\n",
    "        Inputs\n",
    "        - module: non-modified system variable\n",
    "        - grad_in: gradient passed in from last fc layer\n",
    "        - grad_output: gradient passed out from this conv layer\n",
    "\n",
    "        Append gradients generated by this conv layer to a list each time\n",
    "        \"\"\"\n",
    "        gradient_block.append(grad_out[0].detach())\n",
    "\n",
    "    def get_class_loss(out_vec, index=None):\n",
    "        if not index:\n",
    "            index = np.argmax(out_vec.cpu().data.numpy())\n",
    "        else:\n",
    "            index = np.array(index)\n",
    "        index = index[np.newaxis, np.newaxis]\n",
    "        index = torch.from_numpy(index)\n",
    "        one_hot = torch.zeros(1, 8).scatter_(1, index, 1)\n",
    "        one_hot.requires_grad = True\n",
    "        one_hot = one_hot.to(device)\n",
    "        class_loss = torch.sum(one_hot * out_vec)\n",
    "\n",
    "        return class_loss\n",
    "\n",
    "    feature_map_block = list() # feature map\n",
    "    gradient_block = list()\n",
    "\n",
    "    model.train()\n",
    "    # register hook\n",
    "    model.conv4.register_forward_hook(forward_hook)\n",
    "    model.conv4.register_backward_hook(backward_hook)\n",
    "\n",
    "    # forward\n",
    "    output_logits, output_softmax = model(input_image)\n",
    "\n",
    "    # backward\n",
    "    model.zero_grad()\n",
    "    loss = get_class_loss(output_logits)\n",
    "    loss.backward()\n",
    "\n",
    "    # generate afm\n",
    "    gradient = gradient_block[0].cpu().data.numpy().squeeze()\n",
    "    fmap = feature_map_block[0].cpu().data.numpy().squeeze()\n",
    "    afm = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
    "    weights = np.mean(gradient, axis=(1, 2))\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        afm += w * fmap[i, :, :]\n",
    "\n",
    "    # Post processing\n",
    "    afm = np.maximum(afm, 0)\n",
    "    afm = (afm - np.min(afm)) / (np.max(afm) - np.min(afm))  # Normalize between 0-1\n",
    "    afm = np.uint8(afm * 255)  # Scale between 0-255 to visualize\n",
    "    afm = np.uint8(Image.fromarray(afm).resize((input_image.shape[2],\n",
    "                    input_image.shape[3]), Image.ANTIALIAS))\n",
    "\n",
    "    return afm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MgAjMP41BOH"
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXbzUzs2zCEv"
   },
   "outputs": [],
   "source": [
    "emotion = 0\n",
    "index = np.where(Y_train == emotion)\n",
    "index = index[0]\n",
    "\n",
    "heatmap_tensor_sum1 = torch.zeros(4, 148, 188)\n",
    "heatmap_tensor_sum1.to(device)\n",
    "model.to(device)\n",
    "\n",
    "AFM = torch.zeros(188, 148)\n",
    "AFM.to(device)\n",
    "for i in index:\n",
    "    im = X_train[i, :, :, :]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im_tensor = torch.tensor(im, device=device).float()\n",
    "\n",
    "    org_im = np.squeeze(im)\n",
    "    org_im_expand = np.expand_dims(org_im, axis=0)\n",
    "\n",
    "    org_im_tensor = torch.tensor(org_im_expand, device=device).float()\n",
    "    org_im_tensor = inverse_transform(org_im_tensor) # get PIL Image\n",
    "\n",
    "    afm = generate_afm(model, im_tensor)\n",
    "    AFM += afm;\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_im_tensor, afm.T, 'hsv')\n",
    "\n",
    "    heatmap_img_tensor = transform(heatmap_on_image)\n",
    "    heatmap_tensor_sum1 += heatmap_img_tensor\n",
    "\n",
    "heatmap_tensor_sum1 = heatmap_tensor_sum1 / len(index)\n",
    "\n",
    "avgafm = AFM / len(index)\n",
    "\n",
    "heatmap_sum_img1 = inverse_transform(heatmap_tensor_sum1)\n",
    "plt.imshow(heatmap_sum_img1)\n",
    "plt.savefig(\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_results/Visualization_emotion_0.png\")\n",
    "plt.show()\n",
    "\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum0.npy\", arr=heatmap_tensor_sum1)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm0.npy\", arr=avgafm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcOJf_PgzkFc"
   },
   "outputs": [],
   "source": [
    "emotion = 1\n",
    "index = np.where(Y_train == emotion)\n",
    "index = index[0]\n",
    "\n",
    "heatmap_tensor_sum1 = torch.zeros(4, 148, 188)\n",
    "heatmap_tensor_sum1.to(device)\n",
    "model.to(device)\n",
    "\n",
    "AFM = torch.zeros(188, 148)\n",
    "AFM.to(device)\n",
    "for i in index:\n",
    "    im = X_train[i, :, :, :]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im_tensor = torch.tensor(im, device=device).float()\n",
    "\n",
    "    org_im = np.squeeze(im)\n",
    "    org_im_expand = np.expand_dims(org_im, axis=0)\n",
    "\n",
    "    org_im_tensor = torch.tensor(org_im_expand, device=device).float()\n",
    "    org_im_tensor = inverse_transform(org_im_tensor) # get PIL Image\n",
    "\n",
    "    afm = generate_afm(model, im_tensor)\n",
    "    AFM += afm;\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_im_tensor, afm.T, 'hsv')\n",
    "\n",
    "    heatmap_img_tensor = transform(heatmap_on_image)\n",
    "    heatmap_tensor_sum1 += heatmap_img_tensor\n",
    "\n",
    "heatmap_tensor_sum1 = heatmap_tensor_sum1 / len(index)\n",
    "\n",
    "avgafm = AFM / len(index)\n",
    "\n",
    "heatmap_sum_img1 = inverse_transform(heatmap_tensor_sum1)\n",
    "plt.imshow(heatmap_sum_img1)\n",
    "plt.savefig(\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_results/Visualization_emotion_1.png\")\n",
    "plt.show()\n",
    "\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum1.npy\", arr=heatmap_tensor_sum1)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm1.npy\", arr=avgafm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0Ioq-QJvcD0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotion = 2\n",
    "index = np.where(Y_train == emotion)\n",
    "index = index[0]\n",
    "\n",
    "heatmap_tensor_sum1 = torch.zeros(4, 148, 188)\n",
    "heatmap_tensor_sum1.to(device)\n",
    "model.to(device)\n",
    "\n",
    "AFM = torch.zeros(188, 148)\n",
    "AFM.to(device)\n",
    "for i in index:\n",
    "    im = X_train[i, :, :, :]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im_tensor = torch.tensor(im, device=device).float()\n",
    "\n",
    "    org_im = np.squeeze(im)\n",
    "    org_im_expand = np.expand_dims(org_im, axis=0)\n",
    "\n",
    "    org_im_tensor = torch.tensor(org_im_expand, device=device).float()\n",
    "    org_im_tensor = inverse_transform(org_im_tensor) # get PIL Image\n",
    "\n",
    "    afm = generate_afm(model, im_tensor)\n",
    "    AFM += afm;\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_im_tensor, afm.T, 'hsv')\n",
    "\n",
    "    heatmap_img_tensor = transform(heatmap_on_image)\n",
    "    heatmap_tensor_sum1 += heatmap_img_tensor\n",
    "\n",
    "heatmap_tensor_sum1 = heatmap_tensor_sum1 / len(index)\n",
    "\n",
    "avgafm = AFM / len(index)\n",
    "\n",
    "heatmap_sum_img1 = inverse_transform(heatmap_tensor_sum1)\n",
    "plt.imshow(heatmap_sum_img1)\n",
    "plt.savefig(\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_results/Visualization_emotion_2.png\")\n",
    "plt.show()\n",
    "\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum2.npy\", arr=heatmap_tensor_sum1)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm2.npy\", arr=avgafm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pnf_WMWevcD0"
   },
   "outputs": [],
   "source": [
    "emotion = 3\n",
    "index = np.where(Y_train == emotion)\n",
    "index = index[0]\n",
    "\n",
    "heatmap_tensor_sum1 = torch.zeros(4, 148, 188)\n",
    "heatmap_tensor_sum1.to(device)\n",
    "model.to(device)\n",
    "\n",
    "AFM = torch.zeros(188, 148)\n",
    "AFM.to(device)\n",
    "for i in index:\n",
    "    im = X_train[i, :, :, :]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im_tensor = torch.tensor(im, device=device).float()\n",
    "\n",
    "    org_im = np.squeeze(im)\n",
    "    org_im_expand = np.expand_dims(org_im, axis=0)\n",
    "\n",
    "    org_im_tensor = torch.tensor(org_im_expand, device=device).float()\n",
    "    org_im_tensor = inverse_transform(org_im_tensor) # get PIL Image\n",
    "\n",
    "    afm = generate_afm(model, im_tensor)\n",
    "    AFM += afm;\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_im_tensor, afm.T, 'hsv')\n",
    "\n",
    "    heatmap_img_tensor = transform(heatmap_on_image)\n",
    "    heatmap_tensor_sum1 += heatmap_img_tensor\n",
    "\n",
    "heatmap_tensor_sum1 = heatmap_tensor_sum1 / len(index)\n",
    "\n",
    "avgafm = AFM / len(index)\n",
    "\n",
    "heatmap_sum_img1 = inverse_transform(heatmap_tensor_sum1)\n",
    "plt.imshow(heatmap_sum_img1)\n",
    "plt.savefig(\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_results/Visualization_emotion_3.png\")\n",
    "plt.show()\n",
    "\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum3.npy\", arr=heatmap_tensor_sum1)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm3.npy\", arr=avgafm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8062U2fb0TPB"
   },
   "outputs": [],
   "source": [
    "emotion = 4\n",
    "index = np.where(Y_train == emotion)\n",
    "index = index[0]\n",
    "\n",
    "heatmap_tensor_sum1 = torch.zeros(4, 148, 188)\n",
    "heatmap_tensor_sum1.to(device)\n",
    "model.to(device)\n",
    "\n",
    "AFM = torch.zeros(188, 148)\n",
    "AFM.to(device)\n",
    "for i in index:\n",
    "    im = X_train[i, :, :, :]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im_tensor = torch.tensor(im, device=device).float()\n",
    "\n",
    "    org_im = np.squeeze(im)\n",
    "    org_im_expand = np.expand_dims(org_im, axis=0)\n",
    "\n",
    "    org_im_tensor = torch.tensor(org_im_expand, device=device).float()\n",
    "    org_im_tensor = inverse_transform(org_im_tensor) # get PIL Image\n",
    "\n",
    "    afm = generate_afm(model, im_tensor)\n",
    "    AFM += afm;\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_im_tensor, afm.T, 'hsv')\n",
    "\n",
    "    heatmap_img_tensor = transform(heatmap_on_image)\n",
    "    heatmap_tensor_sum1 += heatmap_img_tensor\n",
    "\n",
    "heatmap_tensor_sum1 = heatmap_tensor_sum1 / len(index)\n",
    "\n",
    "avgafm = AFM / len(index)\n",
    "\n",
    "heatmap_sum_img1 = inverse_transform(heatmap_tensor_sum1)\n",
    "plt.imshow(heatmap_sum_img1)\n",
    "plt.savefig(\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_results/Visualization_emotion_4.png\")\n",
    "plt.show()\n",
    "\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum4.npy\", arr=heatmap_tensor_sum1)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm4.npy\", arr=avgafm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYC9m2KC0TgI"
   },
   "outputs": [],
   "source": [
    "emotion = 5\n",
    "index = np.where(Y_train == emotion)\n",
    "index = index[0]\n",
    "\n",
    "heatmap_tensor_sum1 = torch.zeros(4, 148, 188)\n",
    "heatmap_tensor_sum1.to(device)\n",
    "model.to(device)\n",
    "\n",
    "AFM = torch.zeros(188, 148)\n",
    "AFM.to(device)\n",
    "for i in index:\n",
    "    im = X_train[i, :, :, :]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im_tensor = torch.tensor(im, device=device).float()\n",
    "\n",
    "    org_im = np.squeeze(im)\n",
    "    org_im_expand = np.expand_dims(org_im, axis=0)\n",
    "\n",
    "    org_im_tensor = torch.tensor(org_im_expand, device=device).float()\n",
    "    org_im_tensor = inverse_transform(org_im_tensor) # get PIL Image\n",
    "\n",
    "    afm = generate_afm(model, im_tensor)\n",
    "    AFM += afm;\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_im_tensor, afm.T, 'hsv')\n",
    "\n",
    "    heatmap_img_tensor = transform(heatmap_on_image)\n",
    "    heatmap_tensor_sum1 += heatmap_img_tensor\n",
    "\n",
    "heatmap_tensor_sum1 = heatmap_tensor_sum1 / len(index)\n",
    "\n",
    "avgafm = AFM / len(index)\n",
    "\n",
    "heatmap_sum_img1 = inverse_transform(heatmap_tensor_sum1)\n",
    "plt.imshow(heatmap_sum_img1)\n",
    "plt.savefig(\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_results/Visualization_emotion_5.png\")\n",
    "plt.show()\n",
    "\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum5.npy\", arr=heatmap_tensor_sum1)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm5.npy\", arr=avgafm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uixojTXp0ToL"
   },
   "outputs": [],
   "source": [
    "emotion = 6\n",
    "index = np.where(Y_train == emotion)\n",
    "index = index[0]\n",
    "\n",
    "heatmap_tensor_sum1 = torch.zeros(4, 148, 188)\n",
    "heatmap_tensor_sum1.to(device)\n",
    "model.to(device)\n",
    "\n",
    "AFM = torch.zeros(188, 148)\n",
    "AFM.to(device)\n",
    "for i in index:\n",
    "    im = X_train[i, :, :, :]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im_tensor = torch.tensor(im, device=device).float()\n",
    "\n",
    "    org_im = np.squeeze(im)\n",
    "    org_im_expand = np.expand_dims(org_im, axis=0)\n",
    "\n",
    "    org_im_tensor = torch.tensor(org_im_expand, device=device).float()\n",
    "    org_im_tensor = inverse_transform(org_im_tensor) # get PIL Image\n",
    "\n",
    "    afm = generate_afm(model, im_tensor)\n",
    "    AFM += afm;\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_im_tensor, afm.T, 'hsv')\n",
    "\n",
    "    heatmap_img_tensor = transform(heatmap_on_image)\n",
    "    heatmap_tensor_sum1 += heatmap_img_tensor\n",
    "\n",
    "heatmap_tensor_sum1 = heatmap_tensor_sum1 / len(index)\n",
    "\n",
    "avgafm = AFM / len(index)\n",
    "\n",
    "heatmap_sum_img1 = inverse_transform(heatmap_tensor_sum1)\n",
    "plt.imshow(heatmap_sum_img1)\n",
    "plt.savefig(\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_results/Visualization_emotion_6.png\")\n",
    "plt.show()\n",
    "\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum6.npy\", arr=heatmap_tensor_sum1)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm6.npy\", arr=avgafm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79XlyUcN0TuK"
   },
   "outputs": [],
   "source": [
    "emotion = 7\n",
    "index = np.where(Y_train == emotion)\n",
    "index = index[0]\n",
    "\n",
    "heatmap_tensor_sum1 = torch.zeros(4, 148, 188)\n",
    "heatmap_tensor_sum1.to(device)\n",
    "model.to(device)\n",
    "\n",
    "AFM = torch.zeros(188, 148)\n",
    "AFM.to(device)\n",
    "for i in index:\n",
    "    im = X_train[i, :, :, :]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im_tensor = torch.tensor(im, device=device).float()\n",
    "\n",
    "    org_im = np.squeeze(im)\n",
    "    org_im_expand = np.expand_dims(org_im, axis=0)\n",
    "\n",
    "    org_im_tensor = torch.tensor(org_im_expand, device=device).float()\n",
    "    org_im_tensor = inverse_transform(org_im_tensor) # get PIL Image\n",
    "\n",
    "    afm = generate_afm(model, im_tensor)\n",
    "    AFM += afm;\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_im_tensor, afm.T, 'hsv')\n",
    "\n",
    "    heatmap_img_tensor = transform(heatmap_on_image)\n",
    "    heatmap_tensor_sum1 += heatmap_img_tensor\n",
    "\n",
    "heatmap_tensor_sum1 = heatmap_tensor_sum1 / len(index)\n",
    "\n",
    "avgafm = AFM / len(index)\n",
    "\n",
    "heatmap_sum_img1 = inverse_transform(heatmap_tensor_sum1)\n",
    "plt.imshow(heatmap_sum_img1)\n",
    "plt.savefig(\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_results/Visualization_emotion_7.png\")\n",
    "plt.show()\n",
    "\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum7.npy\", arr=heatmap_tensor_sum1)\n",
    "np.save(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm7.npy\", arr=avgafm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5kmbVnT1JHf"
   },
   "outputs": [],
   "source": [
    "heatmap_tensor_sum1 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum1.npy\")\n",
    "heatmap_tensor_sum2 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum2.npy\")\n",
    "heatmap_tensor_sum3 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum3.npy\")\n",
    "heatmap_tensor_sum4 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum4.npy\")\n",
    "heatmap_tensor_sum5 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum5.npy\")\n",
    "heatmap_tensor_sum6 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum6.npy\")\n",
    "heatmap_tensor_sum7 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum7.npy\")\n",
    "heatmap_tensor_sum0 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/heatmap_tensor_sum0.npy\")\n",
    "\n",
    "heatmap_tensor_sum = (heatmap_tensor_sum1\n",
    "                     + heatmap_tensor_sum2\n",
    "                     + heatmap_tensor_sum3\n",
    "                     + heatmap_tensor_sum4\n",
    "                     + heatmap_tensor_sum5\n",
    "                     + heatmap_tensor_sum6\n",
    "                     + heatmap_tensor_sum7\n",
    "                     + heatmap_tensor_sum0) / 8\n",
    "\n",
    "heatmap_tensor_sum = torch.tensor(heatmap_tensor_sum, device=device)\n",
    "\n",
    "heatmap_sum_img = inverse_transform(heatmap_tensor_sum)\n",
    "plt.imshow(heatmap_sum_img)\n",
    "plt.savefig(\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_results/Visualization_AVG_for_ALL.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-e_HH7VY1ka_"
   },
   "outputs": [],
   "source": [
    "avgafm0 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm0.npy\")\n",
    "avgafm1 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm1.npy\")\n",
    "avgafm2 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm2.npy\")\n",
    "avgafm3 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm3.npy\")\n",
    "avgafm4 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm4.npy\")\n",
    "avgafm5 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm5.npy\")\n",
    "avgafm6 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm6.npy\")\n",
    "avgafm7 = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/Training_in_3_datasets/big_ctrl_mel_temp/avgafm7.npy\")\n",
    "\n",
    "afm_sum_avg = (avgafm0\n",
    "         + avgafm1\n",
    "         + avgafm2\n",
    "         + avgafm3\n",
    "         + avgafm4\n",
    "         + avgafm5\n",
    "         + avgafm6\n",
    "         + avgafm7) / 8\n",
    "\n",
    "print(afm_sum_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7Iq4YTs10h6"
   },
   "outputs": [],
   "source": [
    "fam = afm_sum_avg.T\n",
    "\n",
    "Conv = np.zeros([44, 80])\n",
    "for i in range(44):\n",
    "    for j in range(80):\n",
    "        Conv[i, j] = sum(sum(fam[i:i+104, j:j+108]))\n",
    "\n",
    "print(Conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx0Z0_Ct149T"
   },
   "outputs": [],
   "source": [
    "index_max_conv = np.argmax(Conv)\n",
    "max_conv = np.max(Conv)\n",
    "print(index_max_conv)\n",
    "print(max_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pyHtAmqD_k5"
   },
   "outputs": [],
   "source": [
    "# index_min_conv = np.argmin(Conv)\n",
    "# min_conv = np.min(Conv)\n",
    "# print(index_min_conv)\n",
    "# print(min_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGRuU9gM-zc6"
   },
   "outputs": [],
   "source": [
    "row = index_max_conv // 80\n",
    "col = -1 * (row * 80 - index_max_conv)\n",
    "\n",
    "print(row, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loGhbyPeECGH"
   },
   "outputs": [],
   "source": [
    "# row1 = index_min_conv // 80\n",
    "# col1 = -1 * (row1 * 80 - index_min_conv)\n",
    "\n",
    "# print(row1, col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gkhd-y018be"
   },
   "outputs": [],
   "source": [
    "Conv[row, col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt7s6qjeYo_5"
   },
   "outputs": [],
   "source": [
    "# Conv[row1, col1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Smq9PSk1Ee_O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
