{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fYFLFMFXSZk"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HxiWCX5clPM"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bdx9dPOJclV8"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CvOXDYwXSZn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msKiHlDSXSZ3"
   },
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwPmDlUkbpJH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.GELU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.gelu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIX_bfpcXSZ3"
   },
   "outputs": [],
   "source": [
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self, block, layers, num_emotions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Resnet Block\n",
    "        self.inplanes = 64\n",
    "        self.res_conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(1, 64, kernel_size = 3, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.GELU())\n",
    "        self.res_maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.res_layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.res_layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.res_layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.res_layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.res_avgpool = nn.AvgPool2d(3, stride=1)\n",
    "#         self.res_fc = nn.Linear(512, num_emotions)\n",
    "\n",
    "        # Block 1:\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                    out_channels=16,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Block 2:\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,\n",
    "                    out_channels=32,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "\n",
    "        # Block 3:\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "\n",
    "        # Block 4:\n",
    "        self.conv4 = nn.Conv2d(in_channels=64,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.max_pool4 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "\n",
    "        # LSTM block\n",
    "        self.lstm_maxpool = nn.MaxPool2d(kernel_size=[2,4], stride=[2,4])\n",
    "        hidden_size = 148\n",
    "        self.lstm = nn.LSTM(input_size=74,hidden_size=hidden_size,bidirectional=True, batch_first=True)\n",
    "        self.dropout_lstm = nn.Dropout(0.1)\n",
    "        self.attention_linear = nn.Linear(2*hidden_size,1) # 2*hidden_size for the 2 outputs of bidir LSTM\n",
    "\n",
    "        # Transformer block\n",
    "        self.transf_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
    "        transf_layer = nn.TransformerEncoderLayer(d_model=148, nhead=4, dim_feedforward=512, dropout=0.4, activation='relu')\n",
    "        self.transf_encoder = nn.TransformerEncoder(transf_layer, num_layers=4)\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(2*hidden_size+212+15360,num_emotions)\n",
    "        self.dropout_linear = nn.Dropout(p=0)\n",
    "        self.out_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # resnet embedding\n",
    "        res_embedding = self.res_conv1(x)\n",
    "        res_embedding = self.res_maxpool(res_embedding)\n",
    "        res_embedding = self.res_layer0(res_embedding)\n",
    "        res_embedding = self.res_layer1(res_embedding)\n",
    "        res_embedding = self.dropout(res_embedding)\n",
    "        res_embedding = self.res_layer2(res_embedding)\n",
    "        res_embedding = self.res_layer3(res_embedding)\n",
    "        res_embedding = self.dropout(res_embedding)\n",
    "\n",
    "        # res_embedding = self.res_avgpool(res_embedding)\n",
    "        res_embedding = res_embedding.view(res_embedding.size(0), -1)\n",
    "\n",
    "        # conv embedding\n",
    "#         conv_embedding = self.conv2Dblock(x) #(b,channel,freq,time)\n",
    "        conv_embedding = self.dropout(self.max_pool1(self.relu(self.bn1(self.conv1(x)))))\n",
    "        conv_embedding = self.dropout(self.max_pool2(self.relu(self.bn2(self.conv2(conv_embedding)))))\n",
    "        conv_embedding = self.dropout(self.max_pool3(self.relu(self.bn3(self.conv3(conv_embedding)))))\n",
    "        conv_embedding = self.dropout(self.max_pool4(self.relu(self.bn4(self.conv4(conv_embedding)))))\n",
    "\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=1) # do not flatten batch dimension\n",
    "\n",
    "        # lstm embedding\n",
    "        x_reduced = self.lstm_maxpool(x)\n",
    "        x_reduced = torch.squeeze(x_reduced,1)\n",
    "        x_reduced = x_reduced.permute(0,2,1) # (batch,time,freq)\n",
    "        lstm_embedding, (h,c) = self.lstm(x_reduced) # (b, time, hidden_size*2)\n",
    "        lstm_embedding = self.dropout_lstm(lstm_embedding)\n",
    "        batch_size,T,_ = lstm_embedding.shape\n",
    "        attention_weights = [None]*T\n",
    "        for t in range(T):\n",
    "            embedding = lstm_embedding[:,t,:]\n",
    "            attention_weights[t] = self.attention_linear(embedding)\n",
    "        attention_weights_norm = nn.functional.softmax(torch.stack(attention_weights,-1),-1)\n",
    "        attention = torch.bmm(attention_weights_norm,lstm_embedding) # (Bx1xT)*(B,T,hidden_size*2)=(B,1,2*hidden_size)\n",
    "        attention = torch.squeeze(attention, 1)\n",
    "\n",
    "\n",
    "        # transformer embedding\n",
    "        x_reduced_t = self.transf_maxpool(x)\n",
    "        x_reduced_t = torch.squeeze(x_reduced_t,1)\n",
    "        x_reduced_t = x_reduced_t.permute(2,0,1) # (time,batch,embedding)\n",
    "        transf_out = self.transf_encoder(x_reduced_t)\n",
    "        transf_embedding = torch.mean(transf_out, dim=0)\n",
    "        # concatenate\n",
    "        complete_embedding = torch.cat([res_embedding, conv_embedding, attention, transf_embedding], dim=1)\n",
    "        # final Linear\n",
    "        output_logits = self.out_linear(complete_embedding)\n",
    "        output_logits = self.dropout_linear(output_logits)\n",
    "        output_softmax = self.out_softmax(output_logits)\n",
    "        return output_logits, output_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B8BF95qXSZ4"
   },
   "outputs": [],
   "source": [
    "def loss_fnc(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwCWQ3qgXSZ5"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kG-QAU39XSZ5"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fnc, optimizer):\n",
    "    def train_step(X,Y):\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        # forward pass\n",
    "        output_logits, output_softmax = model(X)\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        # compute loss\n",
    "        loss = loss_fnc(output_logits, Y)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters and zero gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tHuRMZHXSZ6"
   },
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,loss_fnc):\n",
    "    def validate(X,Y):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output_logits, output_softmax = model(X)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            loss = loss_fnc(output_logits,Y)\n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtv6tuuVXSZ6"
   },
   "source": [
    "scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tm3hz4yBXSZ6"
   },
   "outputs": [],
   "source": [
    "X_train = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/xtrain.npy\")\n",
    "X_test = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/xtest.npy\")\n",
    "X_val = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/xval.npy\")\n",
    "\n",
    "Y_train = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/ytrain.npy\")\n",
    "Y_test = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/ytest.npy\")\n",
    "Y_val = np.load(file=\"/content/drive/MyDrive/serdl/notebooks/3_dataset_mel/yval.npy\")\n",
    "\n",
    "X_train.shape\n",
    "# random.shuffle(Y_train)\n",
    "# random.shuffle(Y_train)\n",
    "# random.shuffle(Y_train)\n",
    "# random.shuffle(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-Loswb8XSZ6"
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnmdgZiWXSZ7"
   },
   "outputs": [],
   "source": [
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'}\n",
    "EPOCHS=220\n",
    "DATASET_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device is {}'.format(device))\n",
    "model = ParallelModel(ResidualBlock, [2,2,2,2], len(EMOTIONS)).to(device)\n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()))\n",
    "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
    "validate = make_validate_fnc(model,loss_fnc)\n",
    "losses=[]\n",
    "val_losses = []\n",
    "start = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    # schuffle data\n",
    "    start_epoch = time.time()\n",
    "    ind = np.random.permutation(DATASET_SIZE)\n",
    "    X_train = X_train[ind,:,:,:]\n",
    "    Y_train = Y_train[ind]\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    iters = int(DATASET_SIZE / BATCH_SIZE)\n",
    "    for i in range(iters):\n",
    "        batch_start = i * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n",
    "        actual_batch_size = batch_end-batch_start\n",
    "        X = X_train[batch_start:batch_end,:,:,:]\n",
    "        Y = Y_train[batch_start:batch_end]\n",
    "        X_tensor = torch.tensor(X,device=device).float()\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "        loss, acc = train_step(X_tensor,Y_tensor)\n",
    "        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n",
    "        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n",
    "        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n",
    "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
    "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
    "    val_loss, val_acc, predictions = validate(X_val_tensor,Y_val_tensor)\n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    elapsed_epoch = time.time() - start_epoch\n",
    "    print('')\n",
    "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, \"\n",
    "        f\"val_acc:{val_acc:.2f}%, time:{elapsed_epoch:.2f}sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxFlw1cQbpJK"
   },
   "outputs": [],
   "source": [
    "elapsed = time.time() - start\n",
    "print(f\"Total training time:{elapsed:.2f}sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR_nciz5XSZ8"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYh94T3DXSZ9"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(os.getcwd(),'/content/drive/MyDrive/serdl/notebooks/new_models')\n",
    "os.makedirs('models',exist_ok=True)\n",
    "torch.save(model.state_dict(),os.path.join(SAVE_PATH,'big_ctrl_mel_model.pt'))\n",
    "print('Model is saved to {}'.format(os.path.join(SAVE_PATH,'big_ctrl_mel_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dLJPgBrXSZ-"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SxuUb9BXSZ-"
   },
   "outputs": [],
   "source": [
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device is {}'.format(device))\n",
    "\n",
    "LOAD_PATH = os.path.join(os.getcwd(),'/content/drive/MyDrive/serdl/notebooks/new_models')\n",
    "model = ParallelModel(ResidualBlock, [2,2,2,2], len(EMOTIONS))\n",
    "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'big_ctrl_mel_model.pt')))\n",
    "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'big_ctrl_mel_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5cijTORXSZ_"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsVKWvlNXSZ_"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
    "validate = make_validate_fnc(model,loss_fnc)\n",
    "\n",
    "start = time.time()\n",
    "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
    "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
    "test_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\n",
    "elapsed = time.time() - start\n",
    "print(f'Test loss is {test_loss:.3f}')\n",
    "print(f'Test accuracy is {test_acc:.2f}%')\n",
    "print(f\"Total test time:{elapsed:.3f}sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTZ7YtspXSZ_"
   },
   "source": [
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS76pSPtXSZ_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sn\n",
    "\n",
    "predictions = predictions.cpu().numpy()\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions, average='micro')\n",
    "print(\"F1-Score:\", f1)\n",
    "names = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\n",
    "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGx_5F6GXSaB"
   },
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jirP-QhRXSaB"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses,'b')\n",
    "plt.plot(val_losses,'r')\n",
    "plt.legend(['train loss','val loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpGyYHICbpJM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
